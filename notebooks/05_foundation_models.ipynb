{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Foundation Models para Dados Tabulares\n",
                "## Comparacao: XGBoost vs TabPFN vs MITRA\n",
                "\n",
                "Este notebook compara modelos classicos (XGBoost) com **Foundation Models** para dados tabulares.\n",
                "\n",
                "### O que sao Foundation Models Tabulares?\n",
                "\n",
                "| Modelo | Descricao |\n",
                "|--------|----------|\n",
                "| **TabPFN** | Transformer pre-treinado em datasets sinteticos |\n",
                "| **MITRA** | Mixed Synthetic Priors - state-of-the-art da AutoGluon |\n",
                "\n",
                "### Trade-off: Performance vs Interpretabilidade\n",
                "\n",
                "| Modelo | Performance | Interpretabilidade |\n",
                "|--------|-------------|-------------------|\n",
                "| XGBoost + SHAP | Boa | **Excelente** |\n",
                "| TabPFN/MITRA | **Excelente** | Limitada |\n",
                "\n",
                "> **Importante**: Em credit scoring, explicabilidade e requisito regulatorio (LGPD, BACEN)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '../src')\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
                "\n",
                "from credit_scoring.data.loader import load_german_credit\n",
                "from credit_scoring.models.train import load_model\n",
                "\n",
                "print(\"Setup completo!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Carregar dados\n",
                "X, y = load_german_credit(save_raw=False)\n",
                "\n",
                "# Split estratificado\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Treino: {len(X_train)} | Teste: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Baseline: XGBoost (ja treinado)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Carregar modelo XGBoost otimizado\n",
                "xgb_model = load_model('../models/best_model_optuna.joblib')\n",
                "\n",
                "# Predicoes\n",
                "xgb_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
                "xgb_pred = xgb_model.predict(X_test)\n",
                "\n",
                "# Metricas\n",
                "xgb_metrics = {\n",
                "    'Model': 'XGBoost (Optuna)',\n",
                "    'ROC-AUC': roc_auc_score(y_test, xgb_proba),\n",
                "    'F1-Score': f1_score(y_test, xgb_pred),\n",
                "    'Precision': precision_score(y_test, xgb_pred),\n",
                "    'Recall': recall_score(y_test, xgb_pred),\n",
                "    'Interpretable': 'Sim (SHAP)'\n",
                "}\n",
                "\n",
                "print(f\"XGBoost ROC-AUC: {xgb_metrics['ROC-AUC']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. TabPFN\n",
                "\n",
                "TabPFN e um transformer pre-treinado que faz predicao sem necessidade de treino."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Instalacao (se necessario)\n",
                "# !pip install tabpfn\n",
                "\n",
                "try:\n",
                "    from tabpfn import TabPFNClassifier\n",
                "    HAS_TABPFN = True\n",
                "    print(\"TabPFN disponivel!\")\n",
                "except ImportError:\n",
                "    HAS_TABPFN = False\n",
                "    print(\"TabPFN nao instalado. Execute: pip install tabpfn\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tabpfn_metrics = None\n",
                "\n",
                "if HAS_TABPFN:\n",
                "    # Preprocessar dados para TabPFN (apenas valores numericos)\n",
                "    X_train_processed = xgb_model.named_steps['preprocessor'].transform(X_train)\n",
                "    X_test_processed = xgb_model.named_steps['preprocessor'].transform(X_test)\n",
                "    \n",
                "    # TabPFN\n",
                "    tabpfn = TabPFNClassifier(device='cpu', N_ensemble_configurations=16)\n",
                "    tabpfn.fit(X_train_processed, y_train)\n",
                "    \n",
                "    tabpfn_proba = tabpfn.predict_proba(X_test_processed)[:, 1]\n",
                "    tabpfn_pred = tabpfn.predict(X_test_processed)\n",
                "    \n",
                "    tabpfn_metrics = {\n",
                "        'Model': 'TabPFN',\n",
                "        'ROC-AUC': roc_auc_score(y_test, tabpfn_proba),\n",
                "        'F1-Score': f1_score(y_test, tabpfn_pred),\n",
                "        'Precision': precision_score(y_test, tabpfn_pred),\n",
                "        'Recall': recall_score(y_test, tabpfn_pred),\n",
                "        'Interpretable': 'Limitada'\n",
                "    }\n",
                "    \n",
                "    print(f\"TabPFN ROC-AUC: {tabpfn_metrics['ROC-AUC']:.4f}\")\n",
                "else:\n",
                "    print(\"Pulando TabPFN (nao instalado)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. MITRA (AutoGluon)\n",
                "\n",
                "MITRA e o state-of-the-art em foundation models tabulares, desenvolvido pela AWS/AutoGluon."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Instalacao (se necessario)\n",
                "# !pip install autogluon.tabular[mitra]\n",
                "\n",
                "try:\n",
                "    from autogluon.tabular import TabularPredictor\n",
                "    HAS_AUTOGLUON = True\n",
                "    print(\"AutoGluon disponivel!\")\n",
                "except ImportError:\n",
                "    HAS_AUTOGLUON = False\n",
                "    print(\"AutoGluon nao instalado. Execute: pip install autogluon.tabular[mitra]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mitra_metrics = None\n",
                "\n",
                "if HAS_AUTOGLUON:\n",
                "    # Preparar dados\n",
                "    train_data = X_train.copy()\n",
                "    train_data['target'] = y_train.values\n",
                "    \n",
                "    test_data = X_test.copy()\n",
                "    \n",
                "    # Treinar MITRA\n",
                "    predictor = TabularPredictor(\n",
                "        label='target',\n",
                "        eval_metric='roc_auc',\n",
                "        path='../models/mitra_model'\n",
                "    ).fit(\n",
                "        train_data,\n",
                "        hyperparameters={'MITRA': {}},\n",
                "        time_limit=60,\n",
                "        verbosity=1\n",
                "    )\n",
                "    \n",
                "    mitra_proba = predictor.predict_proba(test_data).values[:, 1]\n",
                "    mitra_pred = predictor.predict(test_data).values\n",
                "    \n",
                "    mitra_metrics = {\n",
                "        'Model': 'MITRA (AutoGluon)',\n",
                "        'ROC-AUC': roc_auc_score(y_test, mitra_proba),\n",
                "        'F1-Score': f1_score(y_test, mitra_pred),\n",
                "        'Precision': precision_score(y_test, mitra_pred),\n",
                "        'Recall': recall_score(y_test, mitra_pred),\n",
                "        'Interpretable': 'Limitada'\n",
                "    }\n",
                "    \n",
                "    print(f\"MITRA ROC-AUC: {mitra_metrics['ROC-AUC']:.4f}\")\n",
                "else:\n",
                "    print(\"Pulando MITRA (AutoGluon nao instalado)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Comparacao Final"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Consolidar resultados\n",
                "results = [xgb_metrics]\n",
                "\n",
                "if tabpfn_metrics:\n",
                "    results.append(tabpfn_metrics)\n",
                "if mitra_metrics:\n",
                "    results.append(mitra_metrics)\n",
                "\n",
                "comparison_df = pd.DataFrame(results)\n",
                "comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"COMPARACAO: XGBoost vs Foundation Models\")\n",
                "print(\"=\" * 70)\n",
                "print(comparison_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualizacao\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "models = comparison_df['Model'].tolist()\n",
                "auc_scores = comparison_df['ROC-AUC'].tolist()\n",
                "\n",
                "colors = ['#2ecc71' if 'XGBoost' in m else '#3498db' for m in models]\n",
                "bars = ax.barh(models, auc_scores, color=colors, edgecolor='black')\n",
                "\n",
                "# Adicionar valores\n",
                "for bar, score in zip(bars, auc_scores):\n",
                "    ax.text(score + 0.005, bar.get_y() + bar.get_height()/2, \n",
                "            f'{score:.4f}', va='center', fontweight='bold')\n",
                "\n",
                "ax.set_xlabel('ROC-AUC Score', fontsize=12)\n",
                "ax.set_title('Comparacao: Modelos Classicos vs Foundation Models', fontsize=14, fontweight='bold')\n",
                "ax.set_xlim(0.7, 0.9)\n",
                "ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
                "ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../reports/figures/foundation_models_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Analise de Interpretabilidade\n",
                "\n",
                "### XGBoost: Interpretabilidade Completa com SHAP\n",
                "- Explicacoes globais e locais\n",
                "- Feature importance\n",
                "- Waterfall plots por cliente\n",
                "\n",
                "### Foundation Models: Interpretabilidade Limitada\n",
                "- **Attention weights**: Pode indicar features relevantes, mas nao e intuitivo\n",
                "- **Permutation importance**: Possivel mas computacionalmente caro\n",
                "- **SHAP parcial**: Pode ser aplicado mas com limitacoes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Permutation Importance para Foundation Models (se disponiveis)\n",
                "from sklearn.inspection import permutation_importance\n",
                "\n",
                "if HAS_TABPFN and tabpfn_metrics:\n",
                "    print(\"Calculando Permutation Importance para TabPFN...\")\n",
                "    result = permutation_importance(\n",
                "        tabpfn, X_test_processed, y_test,\n",
                "        n_repeats=5, random_state=42, scoring='roc_auc'\n",
                "    )\n",
                "    \n",
                "    # Obter nomes das features do preprocessor\n",
                "    feature_names = xgb_model.named_steps['preprocessor'].get_feature_names_out()\n",
                "    \n",
                "    importance_df = pd.DataFrame({\n",
                "        'feature': feature_names,\n",
                "        'importance': result.importances_mean\n",
                "    }).sort_values('importance', ascending=False).head(10)\n",
                "    \n",
                "    print(\"\\nTop 10 Features (Permutation Importance - TabPFN):\")\n",
                "    print(importance_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Conclusao e Recomendacao"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\"CONCLUSAO\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(\"\"\"\n",
                "PARA PRODUCAO EM CREDIT SCORING:\n",
                "\n",
                "| Cenario                          | Modelo Recomendado |\n",
                "|----------------------------------|--------------------|\n",
                "| Regulatorio exige explicacoes    | XGBoost + SHAP     |\n",
                "| Prototipo/PoC rapido             | TabPFN/MITRA       |\n",
                "| Ensemble                         | Combinar ambos     |\n",
                "| Performance pura                 | MITRA              |\n",
                "\n",
                "RECOMENDACAO FINAL:\n",
                "- Usar XGBoost como modelo principal (explicabilidade)\n",
                "- Usar Foundation Models como benchmark ou ensemble\n",
                "- Documentar trade-offs para stakeholders\n",
                "\"\"\")\n",
                "\n",
                "# Salvar resultados\n",
                "comparison_df.to_csv('../reports/foundation_models_comparison.csv', index=False)\n",
                "print(\"\\nResultados salvos em: reports/foundation_models_comparison.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}